{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c775f68b",
   "metadata": {},
   "source": [
    "# Colab Runbook: Tesla T4 GPU Benchmark & Recipe Pipeline Demo\n",
    "\n",
    "This Colab runbook shows how to run GPU workloads on a free Tesla T4 instance (when available) and includes quick steps to run a simplified Recipe -> Shopping list pipeline demo (fetch, parse, normalize, mock search, optimize) so you can try the full prototype on Google Colab.\n",
    "\n",
    "Follow the UI steps to request a GPU runtime: Runtime -> Change runtime type -> Hardware accelerator -> GPU. Colab assigns an available GPU; if you specifically need a T4 you should request GPU and verify it with `!nvidia-smi`.\n",
    "\n",
    "Notebook outline:\n",
    "1. Setup and environment info\n",
    "2. Install/Upgrade packages (torch/tensorflow)\n",
    "3. Import libs and set deterministic seeds\n",
    "4. Mount Google Drive and configure WORKDIR\n",
    "5. Verify GPU, CUDA and drivers\n",
    "6. PyTorch GPU matmul benchmark\n",
    "7. TensorFlow GPU matmul benchmark\n",
    "8. Profile GPU workload and measure throughput (GFLOPS)\n",
    "9. Save logs and artifacts to Drive\n",
    "10. Automated sanity tests\n",
    "11. Cleanup and recommended next steps\n",
    "\n",
    "Notes:\n",
    "- This notebook uses large matrix matmul operations (4096x4096) to stress the GPU. If you hit OOM, reduce the matrix size (e.g., 2048).\n",
    "- Tesla T4 availability depends on the free Colab pool; the GPU assigned could be different.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e09c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Print environment info and Python version\n",
    "import sys, platform\n",
    "print('Python:', sys.version)\n",
    "print('Platform:', platform.platform())\n",
    "import os\n",
    "print('PWD:', os.getcwd())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73eecc67",
   "metadata": {},
   "source": [
    "## 2) Install / Upgrade Python packages (PyTorch / TensorFlow)\n",
    "\n",
    "Run the cell below to upgrade pip and install PyTorch (CUDA 11.x wheel) and TensorFlow. The example installs a CUDA-compatible PyTorch wheel; Colab typically ships CUDA and compatible PyTorch builds, so this cell is optional. If you want to save time, skip reinstalling torch and tensorflow and instead import existing versions.\n",
    "\n",
    "The cell will print imported versions after installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9711edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: install packages (optional heavy installs)\n",
    "# You can comment/uncomment installs to save time\n",
    "!python -m pip install --upgrade pip setuptools wheel\n",
    "# Example CUDA wheel install (Colab often already has a compatible torch)\n",
    "# Uncomment the next line to explicitly install a CUDA-enabled torch wheel (may take time)\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install --upgrade tensorflow\n",
    "\n",
    "import torch, tensorflow as tf\n",
    "print('torch', getattr(torch, '__version__', 'not installed'))\n",
    "print('tf', getattr(tf, '__version__', 'not installed'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be597a6c",
   "metadata": {},
   "source": [
    "## 3) Import libraries and set deterministic seeds\n",
    "\n",
    "This cell imports common libraries and sets deterministic seeds for reproducibility. For PyTorch, we seed both CPU and CUDA (if available). For TensorFlow we call tf.random.set_seed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9c8db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: imports and seeds\n",
    "import os, time, random\n",
    "import numpy as np\n",
    "\n",
    "# Try imports, may not be installed if you skipped the install cell\n",
    "try:\n",
    "    import torch\n",
    "except Exception:\n",
    "    torch = None\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except Exception:\n",
    "    tf = None\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "if torch is not None:\n",
    "    torch.manual_seed(42)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(42)\n",
    "if tf is not None:\n",
    "    try:\n",
    "        tf.random.set_seed(42)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print('torch:', getattr(torch, '__version__', None), 'cuda available:', torch is not None and torch.cuda.is_available())\n",
    "print('tf:', getattr(tf, '__version__', None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb67e793",
   "metadata": {},
   "source": [
    "## 4) Mount Google Drive and prepare workspace\n",
    "\n",
    "This cell mounts Google Drive so you can persist logs and model checkpoints. You can skip mounting if you don't need persistence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7808dd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: mount drive (optional)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    WORKDIR = '/content/drive/MyDrive/colab_t4_runs'\n",
    "    os.makedirs(WORKDIR, exist_ok=True)\n",
    "except Exception as e:\n",
    "    print('Google drive mount skipped or not available:', e)\n",
    "    WORKDIR = '/content'\n",
    "\n",
    "print('WORKDIR =', WORKDIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea9e567",
   "metadata": {},
   "source": [
    "## 5) Verify GPU, CUDA and driver versions\n",
    "\n",
    "Run `!nvidia-smi` to confirm the assigned GPU and driver. Use Python checks for torch and TensorFlow GPU visibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80b42d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: GPU checks\n",
    "# Driver-level info\n",
    "print('nvidia-smi output:')\n",
    "try:\n",
    "    !nvidia-smi\n",
    "except Exception as e:\n",
    "    print('nvidia-smi not available:', e)\n",
    "\n",
    "# torch and tf checks\n",
    "try:\n",
    "    import torch\n",
    "    print('torch.cuda.is_available():', torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            print('Device name:', torch.cuda.get_device_name(0))\n",
    "        except Exception:\n",
    "            pass\n",
    "except Exception:\n",
    "    print('torch not installed')\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    print('tf GPUs:', tf.config.list_physical_devices('GPU'))\n",
    "except Exception:\n",
    "    print('tensorflow not installed')\n",
    "\n",
    "# Check nvcc if available\n",
    "try:\n",
    "    !nvcc --version\n",
    "except Exception:\n",
    "    print('nvcc not available')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed57b3f6",
   "metadata": {},
   "source": [
    "## 6) PyTorch GPU workload and timing\n",
    "\n",
    "This cell runs a matmul on the GPU using 4096x4096 matrices. If you run out of memory, reduce the matrix size to 2048 or 1024.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe401c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: PyTorch matmul benchmark\n",
    "import time\n",
    "try:\n",
    "    import torch\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    n = 4096\n",
    "    warmups = 2\n",
    "    iters = 3\n",
    "    print('device:', device)\n",
    "    for _ in range(warmups):\n",
    "        A = torch.randn((n, n), device=device)\n",
    "        B = torch.randn((n, n), device=device)\n",
    "        C = torch.matmul(A, B)\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "    times = []\n",
    "    for i in range(iters):\n",
    "        A = torch.randn((n, n), device=device)\n",
    "        B = torch.randn((n, n), device=device)\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        t0 = time.time()\n",
    "        C = torch.matmul(A, B)\n",
    "        if device.type == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        t = time.time() - t0\n",
    "        times.append(t)\n",
    "        print(f'iter {i+1} elapsed {t:.3f}s')\n",
    "    avg = sum(times)/len(times)\n",
    "    n_ops = 2 * (n**3)\n",
    "    gflops = (n_ops / avg) / 1e9\n",
    "    print('avg', avg, 'GFLOPS', gflops)\n",
    "except Exception as e:\n",
    "    print('PyTorch benchmark skipped or failed:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a667de3",
   "metadata": {},
   "source": [
    "## 7) TensorFlow GPU workload and timing\n",
    "\n",
    "Run a similar matmul using TensorFlow with warm-up runs and timing. If TensorFlow raises OOM, lower the matrix size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff48524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: TensorFlow matmul benchmark\n",
    "import time\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    device_name = '/GPU:0' if tf.config.list_physical_devices('GPU') else '/CPU:0'\n",
    "    print('device for tf:', device_name)\n",
    "    n = 4096\n",
    "    warmups = 2\n",
    "    iters = 3\n",
    "    # warmup\n",
    "    for _ in range(warmups):\n",
    "        with tf.device(device_name):\n",
    "            A = tf.random.normal((n, n))\n",
    "            B = tf.random.normal((n, n))\n",
    "            C = tf.matmul(A, B)\n",
    "    # timed runs\n",
    "    times = []\n",
    "    for i in range(iters):\n",
    "        with tf.device(device_name):\n",
    "            A = tf.random.normal((n, n))\n",
    "            B = tf.random.normal((n, n))\n",
    "            t0 = time.time()\n",
    "            C = tf.matmul(A, B)\n",
    "            # eager: force sync by converting to numpy\n",
    "            try:\n",
    "                _ = C.numpy()\n",
    "            except Exception:\n",
    "                pass\n",
    "            t = time.time() - t0\n",
    "        times.append(t)\n",
    "        print(f'iter {i+1} elapsed {t:.3f}s')\n",
    "    avg = sum(times)/len(times)\n",
    "    n_ops = 2 * (n**3)\n",
    "    gflops = (n_ops / avg) / 1e9\n",
    "    print('avg', avg, 'GFLOPS', gflops)\n",
    "except Exception as e:\n",
    "    print('TensorFlow benchmark skipped or failed:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc3ffaf",
   "metadata": {},
   "source": [
    "## 8) Profile and measure throughput (GFLOPS)\n",
    "\n",
    "This cell calculates GFLOPS from the measured times and optionally logs per-iteration times. For deeper profiling, you can use `torch.utils.bottleneck` or TensorBoard profiler (not included by default here). The cell below reuses recorded times from the PyTorch/TensorFlow benchmark cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f848c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: simple profiling wrapper\n",
    "# This cell assumes you saved timing results in variables `torch_times` and `tf_times` in the previous cells.\n",
    "# We'll provide a fallback computation here if those variables are not present.\n",
    "\n",
    "def compute_gflops(n, avg_time):\n",
    "    n_ops = 2 * (n**3)\n",
    "    return (n_ops / avg_time) / 1e9\n",
    "\n",
    "print('Profiling summary placeholder: run the benchmark cells to get actual timings.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a5e4d4",
   "metadata": {},
   "source": [
    "## 9) Save logs, artifacts and checkpoints to Drive\n",
    "\n",
    "This cell demonstrates saving a short log file and optionally a dummy checkpoint to the `WORKDIR` directory (Drive or /content).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3b01ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: save logs and dummy checkpoint\n",
    "log_text = 'Sample GPU run log\\n'\n",
    "log_text += f'run_time: {time.asctime()}\\n'\n",
    "fn = os.path.join(WORKDIR, 'gpu_log.txt')\n",
    "with open(fn, 'w') as f:\n",
    "    f.write(log_text)\n",
    "print('wrote', fn)\n",
    "\n",
    "# Save a small dummy checkpoint if torch exists\n",
    "try:\n",
    "    if torch is not None:\n",
    "        dummy = {'state': [1,2,3]}\n",
    "        torch.save(dummy, os.path.join(WORKDIR, 'dummy_checkpoint.pt'))\n",
    "        print('saved dummy checkpoint')\n",
    "except Exception as e:\n",
    "    print('checkpoint save failed', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d44668",
   "metadata": {},
   "source": [
    "## 10) Automated sanity tests / unit checks\n",
    "\n",
    "This cell runs a few assertions to ensure GPU availability and correctness of matmul result shapes. Adjust checks for your environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c16ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: automated sanity tests\n",
    "ok = True\n",
    "try:\n",
    "    import torch\n",
    "    if not torch.cuda.is_available():\n",
    "        print('WARNING: torch CUDA not available')\n",
    "        ok = False\n",
    "    else:\n",
    "        print('torch CUDA available, device:', torch.cuda.get_device_name(0))\n",
    "except Exception:\n",
    "    print('torch not available on this runtime')\n",
    "    ok = False\n",
    "\n",
    "# simple math test\n",
    "try:\n",
    "    a = torch.randn((16,16), device=('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "    b = torch.randn((16,16), device=('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "    c = torch.matmul(a,b)\n",
    "    assert c.shape == (16,16)\n",
    "    print('matmul shape OK')\n",
    "except Exception as e:\n",
    "    print('small matmul failed', e)\n",
    "    ok = False\n",
    "\n",
    "print('Sanity checks passed' if ok else 'Sanity checks failed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3de9ae5",
   "metadata": {},
   "source": [
    "## 11) Cleanup: unmount Drive and release GPU resources\n",
    "\n",
    "This cell shows steps to release resources. Use `Runtime -> Restart runtime` in the Colab UI if necessary to fully free the GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e28719f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Cleanup\n",
    "try:\n",
    "    if 'drive' in globals():\n",
    "        try:\n",
    "            drive.flush_and_unmount()\n",
    "            print('Drive unmounted')\n",
    "        except Exception as e:\n",
    "            print('drive unmount failed or not available:', e)\n",
    "except Exception as e:\n",
    "    print('cleanup error', e)\n",
    "\n",
    "# free GPU memory\n",
    "try:\n",
    "    import torch\n",
    "    del globals()['A'] if 'A' in globals() else None\n",
    "    del globals()['B'] if 'B' in globals() else None\n",
    "    del globals()['C'] if 'C' in globals() else None\n",
    "    torch.cuda.empty_cache()\n",
    "    print('freed torch GPU cache')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "print('Recommended: Runtime -> Restart runtime to free all resources')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
