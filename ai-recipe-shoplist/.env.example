# AI Recipe Shoplist Crawler - Configuration
# =================================================================
# This file contains all configuration for the AI Recipe Shoplist Crawler
# Copy this file to .env and configure the provider you want to use

# =================================================================
# GENERAL APPLICATION SETTINGS
# =================================================================

# Logging Configuration
LOG_LEVEL=INFO
DEBUG=false
LOG_FILE_ENABLED=false
LOG_FILE_PATH=logs/app.log

# Web Application Settings
PORT=8000
HOST=0.0.0.0

# =================================================================
# AI PROVIDER CONFIGURATIONS
# =================================================================
# AI Provider Selection (choose one: github, openai, azure, ollama)
PROVIDER=github
# PROVIDER_CHAT_ENABLED=true


# TIKTOKEN_MODEL=gpt-4o
TIKTOKEN_ENCODER=o200k_base

# -----------------------------------------------------------------
# GITHUB MODELS PROVIDER
# -----------------------------------------------------------------
# Free tier with conservative rate limits
# Get your token at: https://github.com/settings/tokens

GITHUB_TOKEN=your_github_token_here
GITHUB_MODEL=gpt-4o-mini
GITHUB_API_URL=https://models.inference.ai.azure.com
GITHUB_MODEL_TIMEOUT=30

# -----------------------------------------------------------------
# OPENAI PROVIDER
# -----------------------------------------------------------------
# Paid service with higher rate limits
# Get your API key at: https://platform.openai.com/api-keys

OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-4o-mini
OPENAI_MAX_TOKENS=2000
OPENAI_TEMPERATURE=0.1

# -----------------------------------------------------------------
# AZURE OPENAI PROVIDER
# -----------------------------------------------------------------
# Enterprise service with highest rate limits
# Configure in Azure Portal: https://portal.azure.com

AZURE_OPENAI_API_KEY=your_azure_openai_api_key_here
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
AZURE_OPENAI_API_VERSION=2024-02-01
AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o-mini

# -----------------------------------------------------------------
# OLLAMA PROVIDER (LOCAL)
# -----------------------------------------------------------------
# Local LLM service - no API key required
# Install Ollama: https://ollama.ai

OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b
OLLAMA_TIMEOUT=120

# =================================================================
# RETRY & RATE LIMITING CONFIGURATION
# =================================================================
# Tenacity-based retry configuration for all providers

# -----------------------------------------------------------------
# GLOBAL RETRY DEFAULTS
# -----------------------------------------------------------------
# These serve as fallback values when provider-specific settings are not defined

DEFAULT_MAX_RETRIES=3              # Number of retry attempts for failed requests
DEFAULT_BASE_DELAY=1.0             # Base delay in seconds for exponential backoff
DEFAULT_MAX_DELAY=60.0             # Maximum delay cap in seconds
DEFAULT_RETRY_MULTIPLIER=2.0       # Exponential backoff multiplier (1s → 2s → 4s → 8s...)
DEFAULT_RPM_LIMIT=15               # Default requests per minute limit

# -----------------------------------------------------------------
# GITHUB MODELS RETRY SETTINGS
# -----------------------------------------------------------------
# Conservative settings due to free tier limitations

GITHUB_MAX_RETRIES=3
GITHUB_BASE_DELAY=1.0
GITHUB_MAX_DELAY=60.0
GITHUB_RETRY_MULTIPLIER=2.0
GITHUB_RPM_LIMIT=15               # Conservative limit for free tier

# -----------------------------------------------------------------
# OPENAI RETRY SETTINGS
# -----------------------------------------------------------------
# Moderate settings for paid tier

OPENAI_MAX_RETRIES=3
OPENAI_BASE_DELAY=1.0
OPENAI_MAX_DELAY=60.0
OPENAI_RETRY_MULTIPLIER=2.0
OPENAI_RPM_LIMIT=60               # Higher limits for paid service

# -----------------------------------------------------------------
# AZURE OPENAI RETRY SETTINGS
# -----------------------------------------------------------------
# Higher settings for enterprise tier

AZURE_MAX_RETRIES=3
AZURE_BASE_DELAY=1.0
AZURE_MAX_DELAY=60.0
AZURE_RETRY_MULTIPLIER=2.0
AZURE_RPM_LIMIT=120              # Highest limits for enterprise service

# -----------------------------------------------------------------
# OLLAMA RETRY SETTINGS (LOCAL)
# -----------------------------------------------------------------
# Aggressive settings for local service

OLLAMA_MAX_RETRIES=3
OLLAMA_BASE_DELAY=0.5            # Faster retries for local service
OLLAMA_MAX_DELAY=30.0            # Lower max delay for local service
OLLAMA_RETRY_MULTIPLIER=1.5      # Gentler backoff for local service
OLLAMA_RPM_LIMIT=0               # No rate limiting for local service (0 = unlimited)

# =================================================================
# WEB FETCHER CONFIGURATION
# =================================================================
# Settings for web content fetching, caching, and storage

# HTTP Request Settings
FETCHER_TIMEOUT=30                    # Request timeout in seconds
FETCHER_MAX_SIZE=10485760            # Maximum content size in bytes (10MB)
FETCHER_USER_AGENT=Mozilla/5.0 (compatible; AI-Recipe-Crawler/1.0; +https://github.com/your-repo)

# Cache Configuration
FETCHER_CACHE_TTL=3600               # Cache TTL in seconds (1 hour)
FETCHER_TMP_FOLDER=tmp/web_cache     # Temporary folder for caching

# Content Persistence (defaults to false for backward compatibility)
FETCHER_ENABLE_CONTENT_SAVING=false  # Enable saving original and cleaned content to disk
FETCHER_ENABLE_CONTENT_LOADING=false # Enable loading cleaned content from disk when available

# Html Cleaning Options
FETCHER_CLEANER_HTML_TO_TEXT=true    # Convert HTML to plain text for AI processing

# =================================================================
# QUICK SETUP EXAMPLES
# =================================================================

# Example 1: GitHub Models (Free)
# AI_PROVIDER=github
# GITHUB_TOKEN=ghp_xxxxxxxxxxxxxxxxxxxx

# Example 2: OpenAI (Paid)
# AI_PROVIDER=openai
# OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxx

# Example 3: Azure OpenAI (Enterprise)
# AI_PROVIDER=azure
# AZURE_OPENAI_API_KEY=xxxxxxxxxxxxxxxxxxxx
# AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/

# Example 4: Ollama (Local)
# AI_PROVIDER=ollama
# OLLAMA_MODEL=llama3.2:3b